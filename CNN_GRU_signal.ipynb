{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Nn98YHhqo-Kr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thanat-Music/CNN-GUR/blob/main/CNN_GRU_signal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3MTM3SwZHtT"
      },
      "outputs": [],
      "source": [
        "# !mv /content/kaggle.json ~/.kaggle/\n",
        "# !kaggle competitions download -c uwb-pose-prediction\n",
        "# !unzip /content/uwb-pose-prediction.zip\n",
        "# # /content/drive/MyDrive/superAI3/signal_Hack "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets evaluate pillow==9.2.0"
      ],
      "metadata": {
        "id": "mqa5G70edI06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OOzdiYlYziY",
        "outputId": "82b56642-af14-4da5-d792-43a58cde5eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Eyh9D3CLaJYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "f_GysdDwMX3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/SuperAi2/datasets/uwb-pose-prediction-v2-image-v1.zip"
      ],
      "metadata": {
        "id": "E8kk1YH2TtYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = glob.glob(\"/content/drive/MyDrive/SuperAi2/signal/signal_train/**/*.png\")\n",
        "test_files = glob.glob(\"/content/drive/MyDrive/SuperAi2/signal/signal_test/*.png\")"
      ],
      "metadata": {
        "id": "vKH4UqN3YvtQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "c23dfaf2-479a-4afb-95c4-fdd829311dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6c96287a8e0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/SuperAi2/signal/signal_train/**/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/SuperAi2/signal/signal_test/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = Image.open(train_files[0]).convert(\"RGB\")\n",
        "display(example)"
      ],
      "metadata": {
        "id": "czXCQZBJaRrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "rsuHk24vOatL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = {\n",
        "    \"train\": T.Compose([\n",
        "        T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=torch.tensor([0.4850, 0.4560, 0.4060]), std=torch.tensor([0.2290, 0.2240, 0.2250]))\n",
        "    ]),\n",
        "    \"test\": T.Compose([\n",
        "        T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=torch.tensor([0.4850, 0.4560, 0.4060]), std=torch.tensor([0.2290, 0.2240, 0.2250]))\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "WQHGld5tbZRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset and transform\n",
        "dataset = ImageFolder(\"/content/train\", transform=transforms[\"train\"])\n",
        "\n",
        "# Define the train-test split ratio and the number of samples in the dataset\n",
        "train_ratio = 0.8\n",
        "num_samples = len(dataset)\n",
        "\n",
        "# Define the number of samples in the train and test dataset\n",
        "num_train_samples = int(0.8 * num_samples)\n",
        "num_test_samples = num_samples - num_train_samples\n",
        "\n",
        "train_dataset, eval_dataset = random_split(dataset, [num_train_samples, num_test_samples])\n",
        "\n",
        "# Create the dataloaders for train and test using the train and test subset\n",
        "# 73 is window number\n",
        "train_loader = DataLoader(train_dataset, batch_size=73, shuffle=True)\n",
        "val_loader = DataLoader(eval_dataset, batch_size=73, shuffle=True)"
      ],
      "metadata": {
        "id": "5Zrzxg0wdSiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "oT54KouERyhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0HXfkxbtek4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "AsJfRswHWjnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUCell(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    An implementation of GRUCell.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
        "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "    \n",
        "    def forward(self, x, hidden):   \n",
        "        x = x.view(-1, x.size(1))\n",
        "        gate_x = self.x2h(x) \n",
        "        gate_h = self.h2h(hidden)\n",
        "\n",
        "        gate_x = gate_x.squeeze()\n",
        "        gate_h = gate_h.squeeze()\n",
        "\n",
        "        i_r, i_i, i_n = gate_x.chunk(3, 1)\n",
        "        h_r, h_i, h_n = gate_h.chunk(3, 1)\n",
        "\n",
        "        resetgate = t.sigmoid(i_r + h_r)\n",
        "        inputgate = t.sigmoid(i_i + h_i)\n",
        "        newgate = t.tanh(i_n + (resetgate * h_n))\n",
        "        \n",
        "        hy = newgate + inputgate * (hidden - newgate)\n",
        "        \n",
        "        return hy"
      ],
      "metadata": {
        "id": "b2hnRTgLWOzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):\n",
        "        super(GRUModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "         \n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        " \n",
        "        self.gru_cell = GRUCell(input_dim, hidden_dim, layer_dim)\n",
        "        \n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Initialize hidden state with zeros\n",
        "        #######################\n",
        "        #  USE GPU FOR MODEL  #\n",
        "        #######################\n",
        "        #print(x.shape,\"x.shape\")100, 28, 28\n",
        "        if t.cuda.is_available():\n",
        "            h0 = Variable(t.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
        "        else:\n",
        "            h0 = Variable(t.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
        "         \n",
        "       \n",
        "        outs = []\n",
        "        \n",
        "        hn = h0[0,:,:]\n",
        "        \n",
        "        for seq in range(x.size(1)):\n",
        "            hn = self.gru_cell(x[:,seq,:], hn) \n",
        "            outs.append(hn)\n",
        "            \n",
        "\n",
        "        out = outs[-1].squeeze()\n",
        "        \n",
        "        out = self.fc(out) \n",
        "        # out.size() --> 100, 10\n",
        "        return out"
      ],
      "metadata": {
        "id": "YHOjQ-C6WbLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN-GRU\n"
      ],
      "metadata": {
        "id": "5cPq7gaxvxjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size, output_dim, kernel_size=3, padding=1, num_layers=3):\n",
        "        super(CNN_GRU, self).__init__()\n",
        "\n",
        "        self.cnn_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_dim, out_channels=32, kernel_size=kernel_size, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernel_size, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=kernel_size, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru_layer = nn.GRU(input_size =784, hidden_size =hidden_size, num_layers =num_layers,  batch_first=True)#,\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN layer\n",
        "        x = self.cnn_layer(x)\n",
        "\n",
        "         # Reshape output for GRU layer\n",
        "        # x = x.permute(0, 3, 1, 2)  # swap channel and sequence length dimensions\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        x = x.view(batch_size, channels, height * width)\n",
        "\n",
        "        # GRU layer\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru_layer(x,h0)\n",
        "        # Decode hidden state of last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "TiFe4OMdv1oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F1 Macro score"
      ],
      "metadata": {
        "id": "L19TltxuLD67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_macro(logits, targets):\n",
        "    preds = t.argmax(logits, dim=1)\n",
        "    return f1_score(targets, preds.cpu().numpy(), average='macro')"
      ],
      "metadata": {
        "id": "xdJDTlH-eZno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "zVdCs5_wWopK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 128\n",
        "num_layers = 1 #for GRU\n",
        "input_dim = 3  # for RGB\n",
        "output_dim = 7\n",
        "lr = 0.0005"
      ],
      "metadata": {
        "id": "tH7HaWVEeWRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_GRU(input_dim, hidden_dim, output_dim ,num_layers=num_layers)\n",
        "optim = t.optim.Adam(model.parameters(), lr=lr)\n",
        "# loss_func = nn.BCELoss()\n",
        "loss_func = t.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "wdW8KMGvWfKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0\n",
        "model = model.to(device)\n",
        "while epoch <= 10:\n",
        "    model.train()\n",
        "    optim.zero_grad()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        pred = model(X_batch)\n",
        "        loss_train = loss_func(pred, y_batch)\n",
        "        loss_train.backward()\n",
        "        optim.step()\n",
        "\n",
        "    model.eval()\n",
        "    with t.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            pred = model(X_batch)\n",
        "            loss_val = loss_func(pred, y_batch)\n",
        "            idx_pred = np.argmax(pred.cpu().detach().numpy(),axis=1)\n",
        "            acc = f1_macro(pred.cpu(), y_batch.cpu())\n",
        "    epoch += 1\n",
        "    loss_train = loss_train.cpu().item()\n",
        "    loss_val = loss_val.cpu().item()\n",
        "    # if epoch % 10 == 0:\n",
        "    print(f'Round: {epoch} || train_loss: {loss_train:.3f} || val_loss: {loss_val:.3f} || val_acc: {acc:.3f}')"
      ],
      "metadata": {
        "id": "Tobk9bCzt6Rv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085f37bb-2470-42db-9cec-09f84be88c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round: 1 || train_loss: 1.843 || val_loss: 1.809 || val_acc: 0.143\n",
            "Round: 2 || train_loss: 1.872 || val_loss: 1.880 || val_acc: 0.177\n",
            "Round: 3 || train_loss: 2.247 || val_loss: 1.853 || val_acc: 0.214\n",
            "Round: 4 || train_loss: 1.459 || val_loss: 1.843 || val_acc: 0.270\n",
            "Round: 5 || train_loss: 1.644 || val_loss: 1.634 || val_acc: 0.170\n",
            "Round: 6 || train_loss: 1.569 || val_loss: 1.843 || val_acc: 0.316\n",
            "Round: 7 || train_loss: 1.446 || val_loss: 1.690 || val_acc: 0.240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "JAODvj8QRpZ2"
      }
    }
  ]
}